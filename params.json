{"name":"Fast sample","tagline":"A script for rapidly sampling a proportion of lines from a file","body":"# fast\\_sample\r\n\r\nfast\\_sample - A script for rapidly sampling a proportion of lines from a file\r\n\r\n# SYNOPSIS\r\n\r\nfast\\_sample \\[options\\] \\[file ...\\]\r\n\r\n    Options:\r\n     -proportion|p   The proportion of lines to sample (e.g. .5 for half.)\r\n     -number|n       The number of lines to sample.\r\n     -header|h       Always print the header for every file\r\n     -seed|s         The random seed, for reproducibility\r\n     -man|m          The full man page\r\n\r\n# OPTIONS\r\n\r\n- **-proportion|p**\r\n\r\n    This is a floating point number between 0 and 1 which determines the proportion\r\n    of lines to sample from the input files.\r\n\r\n- **-number|n**\r\n\r\n    This is an integer of the number of lines to be sampled. Sampling a specific \r\n    number of lines instead of a constant \"coin-flip\" proportion is implemented\r\n    using resevoir sampling. Wikipedia has a great explanation of [reservoir sampling](http://en.wikipedia.org/wiki/Reservoir_sampling),\r\n    but I used the code available [here](http://data-analytics-tools.blogspot.com/2009/09/reservoir-sampling-algorithm-in-perl.html).\r\n\r\n- **-header|h**\r\n\r\n    This is a boolean flag, if it exists then the first line of every file will be\r\n    printed. This is useful for when you want to keep the header of a CSV file.\r\n\r\n- **-seed|s**\r\n\r\n    This is an integer flag, it is the seed passed to the random number generator\r\n    which determines which lines are sampled. If you want to make your research\r\n    reproducible, make sure to specify a seed, and the same lines will always\r\n    be selected.\r\n\r\n- **-man|m**\r\n\r\n    The man page for fast\\_sample.\r\n\r\n# DESCRIPTION\r\n\r\n**fast\\_sample** is a program that allows you to work with a subset of your\r\ndata. Sometimes you have a super large file, and you wish you could just\r\nwork with 5% of the data. **fast\\_sample** let's you do this simply. It also\r\nallows you to sample files reproducibly by simply specifying a random\r\nnumber seed. **fast\\_sample** currently supports line-by-line textual \r\nformats such as CSV, and the DBF format.\r\n\r\n# DEPENDENCIES\r\n\r\n**fast\\_sample** attempts to be as smart as possible about requiring 3rd\r\nparty modules. If you are just going to use it for just sampling out of\r\ntext files (line by line format such as CSV) it should work without the\r\naddition of any 3rd party modules, and any perl (I think going back as\r\nfar as 5.6) will work.\r\n\r\nHowever, if you want to sample binary formats (currently only dbf is\r\nsupported), you will unfortunately need to install two modules. In order\r\nto sample DBFs you need [XBase](https://metacpan.org/pod/XBase) for parsing dbf files, and [Text::CSV](https://metacpan.org/pod/Text::CSV)\r\nin order to have \"correct\" CSV file generation. I could have hand-coded\r\na chintzy CSV generator, but it would be wrong and would handle weird\r\nstuff incorrectly (like embedded newlines.)\r\n\r\nIf you need help installing Perl modules (because you want to use the\r\ndbf file capabilities of fast\\_sample), check out the \"how do I install\r\nperl modules\" documentation available [here](http://www.cpan.org/modules/INSTALL.html).\r\n\r\n# INSTALLATION\r\n\r\n## Mac and Linux\r\n\r\n**fast\\_sample** is very lightweight and requires no 3rd party packages\r\ninstalled other than a default Perl installation. Perl comes installed\r\non OSX and Linux, so for both of those, simply clone the repository\r\nand you should be able to execute it at the command line. If you \r\nwant to have it available just for your user, copy the **fast\\_sample**\r\nscript to your ~/bin as follows: \r\n\r\n    cp fast_sample ~/bin\r\n\r\nif you want it available for all users in the system, copy it to your\r\nsystem /usr/local/bin using the following command:\r\n\r\n    sudo cp fast_sample /usr/local/bin\r\n\r\nMake sure that the directory you copy the script to is in your search\r\npath. So to summarize, a full installation would look as follows:\r\n\r\n    # first we clone the repo\r\n    % git clone https://github.com/earino/fast_sample.git\r\n    Cloning into 'fast_sample'...\r\n    remote: Counting objects: 31, done.\r\n    remote: Compressing objects: 100% (26/26), done.\r\n    remote: Total 31 (delta 14), reused 18 (delta 5), pack-reused 0\r\n    Unpacking objects: 100% (31/31), done.\r\n    Checking connectivity... done.\r\n\r\n    # then we go into the newly cloned directory\r\n    % cd fast_sample\r\n\r\n    # then we copy the script to our /usr/local/bin\r\n    % sudo cp fast_sample /usr/local/bin\r\n    Password:\r\n    %\r\n\r\n## Windows\r\n\r\nI have not installed a perl script on windows in a very long time, so\r\nI unfortunately do not know how to do this. If you want to use \r\n**fast\\_sample** on Windows, drop me a note and I'll figure out how to\r\nget this done :-)\r\n\r\n# PERFORMANCE\r\n\r\n## Text Files\r\n\r\n**fast\\_sample** attempts to be as fast as possible. Sampling should be\r\neffortless even when dealing with huge files.\r\n\r\n    $ ls -alh big.csv\r\n    -rw-r--r--  1 earino  staff   3.1G Feb  7 09:15 big.csv\r\n    $ time fast_sample -h -p .001 big.csv > sampled.csv\r\n\r\n    real    0m5.949s\r\n    user    0m5.209s\r\n    sys     0m0.694s\r\n    $ wc -l big.csv\r\n     12174947 big.csv\r\n    $ wc -l sampled.csv\r\n        12277 sampled.csv\r\n\r\nWhen given an integer count via the -n flag, the system executes the\r\nresevoir sampling algorithm for fair sampling across a stream. I don't\r\nbelieve this is needed for the \"coinflip\" percentage of lines approach,\r\nbut someone better than me at statistics can probably chime in if I'm\r\nwrong. Either way, the reservoir sampling is also relatively fast:\r\n\r\n    $ time ./fast_sample -h -n 3 big.csv > /dev/null\r\n\r\n    real    0m8.615s\r\n    user    0m7.928s\r\n    sys     0m0.685s\r\n\r\n## DBF Files\r\n\r\n**fast\\_sample** has to be clever about DBF files, they are clearly not a\r\nparticularly fast format for linear access, so a simple coinflip approach\r\ndid not work. Current performance seems pretty acceptable. 12 seconds to\r\nsample .001 of a nearly 2 gigabyte dbf file with over 38 million rows.\r\n\r\n    $ ls -alh rp19682011.dbf \r\n    -rw-r--r--@ 1 earino  staff   1.9G Oct 16 14:29 /Users/earino/Downloads/rp19682011.dbf\r\n    $ time ./fast_sample -p .001 -h ~/Downloads/rp19682011.dbf > /dev/null\r\n\r\n    real    0m12.004s\r\n    user    0m3.707s\r\n    sys     0m1.267s\r\n\r\n# AUTHOR\r\n\r\nThe home for **fast\\_sample** is on github at https://github.com/earino/fast_sample\r\n\r\nEduardo Arino de la Rubia <earino@gmail.com>\r\n\r\n# ACKNOWLEDGEMENTS\r\n\r\nReservoir sampling code from [Program-o-Babble](http://data-analytics-tools.blogspot.com/2009/09/reservoir-sampling-algorithm-in-perl.html). \r\nMotivation to implement reservoir sampling in the first place provided by [Neal Fultz](https://github.com/nfultz).","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}